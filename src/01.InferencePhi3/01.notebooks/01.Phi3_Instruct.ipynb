{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install onnxruntime-genai==0.6.0 -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [],
   "source": [
    "# Import the onnxruntime_genai library, which is used for running generative AI models with ONNX Runtime\n",
    "import onnxruntime_genai as og\n",
    "\n",
    "# Import the argparse library to handle command-line arguments\n",
    "import argparse\n",
    "\n",
    "# Import the time library to work with time-related functions\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the Model class from the onnxruntime_genai library\n",
    "# The model is loaded from the specified path, which points to a Phi-3-mini-128k-instruct-onnx model\n",
    "# The model is optimized for CPU and mobile devices with specific configurations (int4 precision, rtn block size 32, accuracy level 4)\n",
    "model = og.Model('../../Models/Phi-3.5-mini-instruct-onnx/cpu_and_mobile/cpu-int4-awq-block-128-acc-level-4')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the Tokenizer class from the onnxruntime_genai library\n",
    "# The tokenizer is initialized with the previously loaded model\n",
    "tokenizer = og.Tokenizer(model)\n",
    "\n",
    "# Create a stream for tokenizing input data using the tokenizer instance\n",
    "tokenizer_stream = tokenizer.create_stream()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a dictionary named search_options to store configuration settings for the model's search behavior\n",
    "search_options = {\n",
    "    # Set the maximum length of the generated output to 1024 tokens\n",
    "    \"max_length\": 1024,\n",
    "    \n",
    "    # Set the temperature parameter to 0.6, which controls the randomness of the output\n",
    "    # Lower values make the output more deterministic, while higher values make it more random\n",
    "    \"temperature\": 0.6\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the GeneratorParams class from the onnxruntime_genai library\n",
    "# The parameters are initialized with the previously loaded model\n",
    "params = og.GeneratorParams(model)\n",
    "\n",
    "# Optionally, enable the use of CUDA graphs for optimized performance with a maximum batch size of 1\n",
    "# This line is currently commented out\n",
    "# params.try_use_cuda_graph_with_max_batch_size(1)\n",
    "\n",
    "# Set the search options for the generator parameters using the previously defined search_options dictionary\n",
    "params.set_search_options(**search_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a prompt string that sets up a conversation between the user and the AI assistant\n",
    "# The prompt includes special tokens to indicate the roles of the system, user, and assistant\n",
    "prompt = \"<|system|>You are a helpful AI assistant.<|end|><|user|>Can you introduce yourself?<|end|><|assistant|>\"\n",
    "\n",
    "# Encode the prompt string into input tokens using the tokenizer\n",
    "input_tokens = tokenizer.encode(prompt)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the Generator class from the onnxruntime_genai library\n",
    "# The generator is initialized with the previously loaded model and the configured parameters\n",
    "generator = og.Generator(model, params)\n",
    "\n",
    "generator.append_tokens(input_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Hello! I'm Phi, an AI language model created by Microsoft. I exist purely in the digital realm and don't have personal experiences, but I'm here to help answer your questions and assist with any information or tasks you need. How can I help you today?ï¿½"
     ]
    }
   ],
   "source": [
    "# Loop until the generator has finished generating tokens\n",
    "while not generator.is_done():\n",
    "\n",
    "\n",
    "    generator.generate_next_token()\n",
    "\n",
    "    new_token = generator.get_next_tokens()[0]\n",
    "    token_text = tokenizer.decode(new_token)\n",
    "\n",
    "\n",
    "    print(token_text, end='', flush=True)\n",
    "    \n",
    "    # Decode the new token into a readable string and print it\n",
    "    # The 'end' parameter ensures the output is printed on the same line\n",
    "    # The 'flush' parameter forces the output to be printed immediately\n",
    "    # print(tokenizer_stream.decode(new_token), end='', flush=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  },
  "polyglot_notebook": {
   "kernelInfo": {
    "defaultKernelName": "csharp",
    "items": [
     {
      "aliases": [],
      "name": "csharp"
     }
    ]
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
